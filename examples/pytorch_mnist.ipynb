{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=64,num_workers=1, pin_memory=True)\n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      " - Numpy Shape: (60000, 28, 28)\n",
      " - Tensor Shape: torch.Size([60000, 28, 28])\n",
      " - Transformed Shape: torch.Size([28, 60000, 28])\n",
      " - min: 0.0\n",
      " - max: 1.0\n",
      " - mean: 0.13066047740240005\n",
      " - std: 0.3081078089011192\n",
      " - var: 0.0949304219058486\n"
     ]
    }
   ],
   "source": [
    "train_data = train.train_data\n",
    "train_data = train.transform(train_data.numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train.train_data.size())\n",
    "print(' - Transformed Shape:', train_data.size())\n",
    "print(' - min:', torch.min(train_data))\n",
    "print(' - max:', torch.max(train_data))\n",
    "print(' - mean:', torch.mean(train_data))\n",
    "print(' - std:', torch.std(train_data))\n",
    "print(' - var:', torch.var(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 548)\n",
    "        self.bc1 = nn.BatchNorm1d(548)\n",
    "        \n",
    "        self.fc2 = nn.Linear(548, 252)\n",
    "        self.bc2 = nn.BatchNorm1d(252)\n",
    "        \n",
    "        self.fc3 = nn.Linear(252, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 784))\n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        out = F.log_softmax(h)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "#model.cuda() # CUDA!\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=784, out_features=548)\n",
       "  (bc1): BatchNorm1d(548, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc2): Linear(in_features=548, out_features=252)\n",
       "  (bc2): BatchNorm1d(252, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (fc3): Linear(in_features=252, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abc/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/__main__.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 0 [57664/60000 (96%)]\tLoss: 0.168020\n",
      " Train Epoch: 1 [57664/60000 (96%)]\tLoss: 0.065827\n",
      " Train Epoch: 2 [57664/60000 (96%)]\tLoss: 0.063795\n",
      " Train Epoch: 3 [57664/60000 (96%)]\tLoss: 0.125165\n",
      " Train Epoch: 4 [57664/60000 (96%)]\tLoss: 0.065083\n",
      " Train Epoch: 5 [57664/60000 (96%)]\tLoss: 0.276786\n",
      " Train Epoch: 6 [57664/60000 (96%)]\tLoss: 0.021553\n",
      " Train Epoch: 7 [57664/60000 (96%)]\tLoss: 0.042198\n",
      " Train Epoch: 8 [57664/60000 (96%)]\tLoss: 0.012439\n",
      " Train Epoch: 9 [57664/60000 (96%)]\tLoss: 0.044420\n",
      " Train Epoch: 10 [57664/60000 (96%)]\tLoss: 0.169852\n",
      " Train Epoch: 11 [57664/60000 (96%)]\tLoss: 0.079287\n",
      " Train Epoch: 12 [57664/60000 (96%)]\tLoss: 0.036890\n",
      " Train Epoch: 13 [57664/60000 (96%)]\tLoss: 0.024434\n",
      " Train Epoch: 14 [57664/60000 (96%)]\tLoss: 0.030217\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(15):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Get Samples\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model(data) \n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(y_pred, target)\n",
    "        losses.append(loss.data[0])\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Display\n",
    "        if batch_idx % 100 == 1:\n",
    "            print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * len(data), \n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                loss.data[0]), \n",
    "                end='')\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
